# DPO Configuration for BART-base
model:
  name: "facebook/bart-base"
  size: "base"
  type: "bart"
  max_input_length: 512
  max_target_length: 512

lora:
  r: 16
  lora_alpha: 32
  lora_dropout: 0.1
  target_modules: ["q_proj", "v_proj"]
  task_type: "SEQ_2_SEQ_LM"

neutrality_classifier:
  model_path: outputs/models/bert_corrected/final_model
  loss_weight: 0.3
  target_rate: 0.8
  batch_size: 16
  max_length: 512
  device: cpu  # keep inference off the training GPU if memory is tight


dpo:
  beta: 0.1    # KL penalty coefficient (lower = more aggressive preference learning)

training:
  learning_rate: 1.0e-5  # Lower LR for DPO (finetuning from SFT)
  per_device_train_batch_size: 4
  per_device_eval_batch_size: 4
  gradient_accumulation_steps: 8
  num_train_epochs: 10
  warmup_ratio: 0.1
  weight_decay: 0.01
  max_grad_norm: 1.0
  fp16: true
  logging_steps: 50
  eval_steps: 200
  save_steps: 400
  evaluation_strategy: "steps"
  save_strategy: "steps"
  load_best_model_at_end: true
  dataloader_num_workers: 4

wandb:
  project: "italian-hyperpartisan-neutralization"
  entity: null
  tags: ["dpo", "bart", "base"]
