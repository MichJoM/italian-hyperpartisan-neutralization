model_id: swiss-ai/Apertus-8B-Instruct-2509
model_name: apertus-8b-instruct
init_args:
  device_map: "auto"
  dtype: "bfloat16"
  attn_implementation: "flash_attention_2"
  # 4-bit quantization enabled for running on smaller GPUs (e.g., A100 MIG, Colab, or local dev) - disable for full run via SLURM.
  # quantization_config:
  # load_in_8bit: True
  # load_in_4bit: True
  # bnb_4bit_compute_dtype: "bfloat16"
  # bnb_4bit_use_double_quant: True
  # bnb_4bit_quant_type: "nf4"
gen_args:
  max_new_tokens: 2048
  do_sample: True
  temperature: 0.7
  top_p: 0.9
  use_cache: True
token_args:
  padding_side: left
  padding: true
  truncation: true
  max_length: 2048
