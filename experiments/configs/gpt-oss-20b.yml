# nb: use gpu_h100
model_name: "gpt-oss-20b"
# model_id: "openai/gpt-oss-20b"
model_path: "/home/2017025/PARTAGE/LLMs/models/gpt-oss-20b"
token_args:
  #   return_tensors: "pt"
  truncation: True
  padding: True
  #   pad_token: "<eos>"
  #   padding_side: "left"
  max_length: 2048
init_args:
  tensor_parallel_size: 2
  # device_map: "auto"
  # dtype: "bfloat16"
  # dtype: "auto"
  # attn_implementation: "flash_attention_2"
  # For CUDA: float16 by default. Uncomment below for 4-bit quantization (bnb/nf4):
  # quantization_config:
  #   load_in_4bit: True
  #   bnb_4bit_compute_dtype: "bfloat16"
  #   bnb_4bit_quant_type: "nf4"
  #   bnb_4bit_use_double_quant: True
gen_args:
  # max_new_tokens: 2048
  max_tokens: 2048
  # do_sample: True
  temperature: 0.7
  top_p: 0.9
  # use_cache: True #enbaled in HF, incampatible with vLLM.
# decode_args:
# skip_special_tokens: True
