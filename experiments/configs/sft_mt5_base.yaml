model:
  name: "google/mt5-base"
  size: "base"
  type: "t5"
  max_input_length: 512
  max_target_length: 512

lora:
  enabled: true
  r: 16
  lora_alpha: 32
  lora_dropout: 0.1
  target_modules: ["q", "k", "v", "o", "wi", "wo"]
  task_type: "SEQ_2_SEQ_LM"

training:
  neutrality_guidance_weight: 1.5
  neutrality_guidance_frequency: 5
  learning_rate: 5.0e-5
  per_device_train_batch_size: 2
  per_device_eval_batch_size: 4
  gradient_accumulation_steps: 16
  num_train_epochs: 15
  warmup_ratio: 0.1
  weight_decay: 0.01
  max_grad_norm: 1.0
  label_smoothing_factor: 0.1
  fp16: true
  logging_steps: 50
  eval_steps: 200
  save_steps: 400
  evaluation_strategy: "steps"
  save_strategy: "steps"
  load_best_model_at_end: true
  metric_for_best_model: "eval_bleu"
  greater_is_better: true
  early_stopping_patience: 10
  early_stopping_threshold: 0.01
  predict_with_generate: true
  generation_max_length: 512
  generation_num_beams: 4
  dataloader_num_workers: 4

generation:
  max_length: 512
  num_beams: 4
  length_penalty: 1.1
  no_repeat_ngram_size: 3
  early_stopping: true
  temperature: 1.0
  do_sample: false

neutrality_classifier:
  model_path: outputs/models/bert_corrected/final_model
  loss_weight: 1.5
  target_rate: 0.85
  batch_size: 16
  max_length: 256
  device: cpu
  neutral_label: "neutral"
  hyper_label: "hyperpartisan"

callbacks:
  generation_sampler:
    num_samples: 8
    sample_every: 200

wandb:
  project: "italian-hyperpartisan-neutralization"
  entity: null
  tags: ["sft", "mt5", "base", "neutrality-guided"]
