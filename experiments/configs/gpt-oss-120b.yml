# nb: use gpu_h100
model_name: "gpt-oss-120b"
# model_id: "openai/gpt-oss-120b"
model_path: "/home/2017025/PARTAGE/LLMs/models/gpt-oss-120b"
token_args:
  #   return_tensors: "pt"
  truncation: True
  padding: True
  #   pad_token: "<eos>"
  #   padding_side: "left"
  max_length: 2048
init_args:
  # device_map: "auto"
  # device_map: "auto"
  # dtype: "auto"
  # dtype: "bfloat16"
  # attn_implementation: "flash_attention_2"
  # NB: manually edited out the mxfp4 section of the default config.
  # quantization_config:
  # load_in_8bit: True
  #   modules_to_not_convert:
  #     - model.layers.*.self_attn
  #     - model.layers.*.mlp.router
  #     - model.embed_tokens
  #     - lm_head
  # quant_method: "mxfp4"
  tensor_parallel_size: 4
gen_args:
  # max_new_tokens: 2048
  max_tokens: 2048
  # do_sample: True
  temperature: 0.7
  top_p: 0.9
  # use_cache: True # enabled in HF, incompatible with vLLM.
# decode_args:
# skip_special_tokens: True
