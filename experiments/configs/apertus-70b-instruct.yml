# model_id: swiss-ai/Apertus-70b-instruct-2509
model_name: apertus-70b-instruct
model_path: /home/2017025/PARTAGE/LLMs/models/apertus-70b-instruct
init_args:
  # device_map: "auto"
  # dtype: "auto"
  # attn_implementation: "flash_attention_2"
  # 4-bit quantization enabled for running on smaller GPUs (e.g., A100 MIG, Colab, or local dev) - disable for full run via SLURM.
  # quantization_config:
  # load_in_4bit: True
  # bnb_4bit_compute_dtype: "bfloat16"
  # bnb_4bit_use_double_quant: True
  # bnb_4bit_quant_type: "nf4"
  # gpu_memory_utilization: 0.9
  # max_seq_len: 2048 #setting to manage kv cache under vllm.
  tensor_parallel_size: 4 # for 6 gpus on single node.
gen_args:
  # max_new_tokens: 2048
  max_tokens: 2048
  # do_sample: True
  temperature: 0.7
  top_p: 0.9
  # use_cache: True
token_args:
  padding_side: left
  padding: true
  truncation: true
  # max_length: 2048
# decode_args:
# skip_special_tokens: True
